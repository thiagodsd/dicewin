{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "revisao-e-snippets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagodsd/dicewin/blob/master/notebooks_annotations/revisao_e_snippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I-pizlmYGDz",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX3-taG6tmHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHemfsKVf3SF",
        "colab_type": "text"
      },
      "source": [
        "## 1. Classical Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg8EyLNzgPwk",
        "colab_type": "text"
      },
      "source": [
        "### Supervised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBKidjQfklvT",
        "colab_type": "text"
      },
      "source": [
        "#### Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07PTAU6Pknrd",
        "colab_type": "text"
      },
      "source": [
        "#### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-AY8hHYoNnT",
        "colab_type": "text"
      },
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XPPa_C3pHFe",
        "colab_type": "text"
      },
      "source": [
        "IMPORTANTISSIMO: a `cross_val_score` retorna o score no **teste** sempre! Entao algo como\n",
        "\n",
        "```python\n",
        "scr_f1 = cross_val_score(SVC(kernel='linear', C=1), df.drop(labels=['target'], axis=1), df['target'], cv=10, scoring='f1')\n",
        "\n",
        "display(scr_f1)\n",
        "display(np.mean(scr_f1))\n",
        "```\n",
        "\n",
        "Vai dar problema se uma questao pede score no **treino**. Um jeito mais geral de fazer o cross-validation esta escrito abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWktHHe-oPXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm             import SVC\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics         import f1_score\n",
        "\n",
        "df = pd.read_csv('{}/itub/Questoes/classificacao_1.csv'.format(PATH))\n",
        "df.head()\n",
        "\n",
        "scores = list()\n",
        "\n",
        "X = df.drop(labels=['target'], inplace=False, axis=1)\n",
        "y = df['target']\n",
        "\n",
        "for train_id, test_id in KFold(n_splits=10).split(df):\n",
        "  X_train, X_test = X.iloc[train_id], X.iloc[test_id]\n",
        "  y_train, y_test = y.iloc[train_id], y.iloc[test_id]\n",
        "\n",
        "  svc = SVC(kernel='linear', C=1)\n",
        "  svc.fit(X_train, y_train)\n",
        "  \n",
        "  # NAS PARTICOES DE TREINO !\n",
        "  # y_pred = svc.predict(X_test)\n",
        "  # scores.append(f1_score(y_test, y_pred))\n",
        "\n",
        "  y_pred = svc.predict(X_train)\n",
        "  scores.append(f1_score(y_train, y_pred))\n",
        "\n",
        "display(scores)\n",
        "display(np.mean(scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqwqFsb1gP2g",
        "colab_type": "text"
      },
      "source": [
        "### Unsupervised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8reHXlegP7c",
        "colab_type": "text"
      },
      "source": [
        "#### Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m681AMWYxto",
        "colab_type": "text"
      },
      "source": [
        "##### Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMlAhVzuY2zS",
        "colab_type": "text"
      },
      "source": [
        "Strategies for hierarchical clustering generally fall into two types:\n",
        "\n",
        "**Agglomerative**, a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. **Divisive**, a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy [ref](https://www.kaggle.com/vipulgandhi/hierarchical-clustering-explanation).\n",
        "\n",
        "Agglomerative clustering possuem algumas formas diferentes de medir dissimilaridade entre grupos, e isso define a variante do metodo.<br/>\n",
        "**single link** onde a distancia minima entre os membros de cada par de grupos e a medida similaridade; capaz de separar coisas nao-elipticas, mas fragil com respeito a ruido entre clusters.<br/>\n",
        "**complete link** em que a distancia maxima entre os membros e a medida de dissimilaridade; lida bem com ruido, mas ha um vies globular nessa abordagem, alem da tendencia de quebrar grandes clusters.<br/>\n",
        "**average link** em que a distancias entre os centroides e a medida de dissimilaridade; lida bem com ruido, mas tambem possui vies globular [ref](https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec).\n",
        "\n",
        "Do Divisive clustering surge o **bisecting K-Means**, em que sucessivos 2-Means sao aplicados em cada cluster maior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYWEUy8pmS5-",
        "colab_type": "text"
      },
      "source": [
        "##### K-means\n",
        "\n",
        "[ref](https://www.kaggle.com/vipulgandhi/kmeans-detailed-explanation) com detalhes.\n",
        "\n",
        "* K-Medoids\n",
        "\n",
        "Em geral K-means usa a squared euclidian distance -- provavelmente para evitar o calculo da raiz quadrada. Isso torna o K-means menos robusto a outliers. A proposta do K-Medoids e corrigir isso. A ideia central e encontrar objetos representativos no conjunto de dados [ref](https://www.youtube.com/watch?v=GApaAnGx3Fw). Uma vez definidos os objetos representativos -- medoids -- a atualizacao acontece pela verificacao se algum outro objeto diminui a soma de medidas de similaridade. E mais caro que o K-Means, porem mais robusto.\n",
        "\n",
        "* K-Means++\n",
        "\n",
        "Inicializacoes ruins geram agrupamentos ruins no K-means -- alem de eventualmente aumentar o tempo necessario para convergencia --, entao o K-Means++ tenta corrigir isso, gerando inicializacoes quasi-otimas. A ideia consiste em escolher o primeiro centroide aleatoriamente entre os pontos a serem agrupados e a partir do segundo centroide o posicionamento e feito aleatoriamente de acordo com a probabilidade proporcional ao quadrado da distancia dos centroides existentes. Isso garante que **os centroides sejam inicializados de modo mais distante entre si**. [ref](https://medium.com/machine-learning-algorithms-from-scratch/k-means-clustering-from-scratch-in-python-1675d38eee42)\n",
        "\n",
        "Com respeito a outliers, qualquer metodo de inicializacao e otimizacao vai ser sensivel a outliers dado que a funcao objetivo e a soma de quadrados.\n",
        "\n",
        "* Mini Batch K-Means\n",
        "\n",
        "Mais rapido.\n",
        "\n",
        "* Streaming K-Means\n",
        "\n",
        "Versao online do K-means -- e portanto analoga ao Mini Batch K-Means -- em que novos dados vao surgindo e a partir dos quais a posicao dos clusteres vai sendo atualizadas. Eventualmente e possivel usar hiper-parametros para diminuir a importancia de dados muito antigos alem de ser possivel remover clusteres nao atualizados ha muito tempo. [ref](https://stats.stackexchange.com/questions/222235/explain-streaming-k-means)\n",
        "\n",
        "* K-means vs K-medians\n",
        "\n",
        "Usa mediana no lugar da raiz da distancia euclidiana, entao troca uma metrica $\\ell^2$ por uma $\\ell$.\n",
        "\n",
        "* Elbow vs Silhoutte\n",
        "\n",
        "**Elbow**: Inertia $\\times$ k-esimo cluster.<br/> \n",
        "**Silhouette**: The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation) [ref](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb). \n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d80ab22fb291b347b2d9dc3cc7cd614f6b15479)\n",
        "\n",
        "onde $a(i)$ e a media entre i e todos os pontos, representando quao bem o i-esimo ponto esta associado **ao seu cluster**; $b(i)$ e a menor distancia entre e todos os outros pontos **dos outros clusters**, representando a dissimilaridade.\n",
        "\n",
        "Em particular para o caso de GMM a BIC pode ser uma metrica mais adequada para escolha do numero otimo de clusters.\n",
        "\n",
        "* Inertia vs Distortion\n",
        "\n",
        "**Inertia** e a soma das distancias quadraticas entre as amostras o cluster mais proximo. **Distortion** e a media das distancias quadraticas dos centroides aos elementos associados a eles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "824TlaVHQ-LN",
        "colab_type": "text"
      },
      "source": [
        "- - -\n",
        "\n",
        "Snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA48BUl8mVHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "\n",
        "agr      = pd.read_csv('{}/itub/Questoes/agrupamento.csv'.format(PATH))\n",
        "centroid = pd.read_csv('{}/itub/Questoes/centroides_iniciais.csv'.format(PATH))\n",
        "\n",
        "display(agr.head())\n",
        "display(agr.shape)\n",
        "display(centroid.head())\n",
        "\n",
        "# elbow method setting cluster centroids\n",
        "\n",
        "inertia = list()\n",
        "for k in range(1, 16):\n",
        "  kmeans = KMeans(n_clusters = k)\n",
        "  kmeans.cluster_centers_ = centroid.iloc[:k ,:].as_matrix()\n",
        "  kmeans.fit(agr)\n",
        "  inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot([i for i in range(1,16)], inertia, '--.')\n",
        "\n",
        "# listing inertia\n",
        "\n",
        "for i in range(1,16):\n",
        "  print(i, inertia[i-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9gWT-xF14uD",
        "colab_type": "text"
      },
      "source": [
        "##### Mean Shift\n",
        "\n",
        "[ref](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/), [ref](http://efavdb.com/mean-shift/) com mais detalhes sobre a regra de atualizacao dos pontos, e [ref](https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/) com aplicacao em image segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTbPnJUPgQAB",
        "colab_type": "text"
      },
      "source": [
        "#### Pattern Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LZqt5p5gQEq",
        "colab_type": "text"
      },
      "source": [
        "#### Dimension Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rp1bHmQf4cQ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Ensemble Methods\n",
        "\n",
        "Em geral ensemble learning e a aprendizagem a partir da forma de uma combinacao linear de modelos base \n",
        "\n",
        "$$f(y|x,\\pi) = \\sum_{m \\in \\mathcal{M}} w_m f_m(y|x)$$\n",
        "\n",
        "Observacoes:\n",
        "1. ha uma relacao funcional bem clara entre ensemble learning e adaptive basis-function learning, portanto **boosting tambem podem ser pensandos como ensemble learning** em que os pesos sao determinados sequencialmente, uma vez que boosting podem ser pensados como uma forma de fittar adaptives basis-function models de forma greedy. \n",
        "\n",
        "$$f(x) = w_0 + \\sum_m^M w_m \\phi_m (x)$$\n",
        "\n",
        "2. ha uma relacao aparente entre ensemble learning e **neural networks**, em que $f_m$ e analogo a m-esima camada oculta e $w_m$ sao os pesos das camadas output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZofUAblsjV",
        "colab_type": "text"
      },
      "source": [
        "### Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PToWcpkBlwIh",
        "colab_type": "text"
      },
      "source": [
        "### Stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR7wOXu0lygJ",
        "colab_type": "text"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkcwBmt4f42q",
        "colab_type": "text"
      },
      "source": [
        "## 3. Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuDyQ39Tf5Qa",
        "colab_type": "text"
      },
      "source": [
        "## 4. Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jThrEQXff5Yq",
        "colab_type": "text"
      },
      "source": [
        "- - -\n",
        "\n",
        "# In-Depth Topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVp81NkdUw0X",
        "colab_type": "text"
      },
      "source": [
        "## Learning Paradigms\n",
        "\n",
        "Beyond Supervised vs Unsupervised."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lgNOkECUyiw",
        "colab_type": "text"
      },
      "source": [
        "**Batch vs Online Learning**\n",
        "\n",
        "_Batch learning_ é o aprendizado feito de uma vez com todo o conjunto de dados disponível, de modo que se algo precisa ser atualizado -- um novo tipo de spam -- é necessário treinar tudo novamente, novo spam jutamente com todos os antigos treinados anteriormente.\n",
        "\n",
        "_Online learning_ é o aprendizado incrimental, observação a observação ou através de mini-batches. \n",
        "\n",
        "- - -\n",
        "\n",
        "**Instance-Based vs Model-Based Learning**\n",
        "\n",
        "_Instance-Based_ generaliza novos casos a partir dos casos aprendidos, através de alguma métrica de similaridade.\n",
        "\n",
        "_Model-Based_ generaliza novos casos através dos parâmetros de um modelo construído a priori."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9UIk1QJuQrq",
        "colab_type": "text"
      },
      "source": [
        "## Bias-Variance Tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABNo-72ff680",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH37167Yf7Lw",
        "colab_type": "text"
      },
      "source": [
        "## Validation\n",
        "\n",
        "Dado um conjunto de dados $\\mathcal{D}$\n",
        "\n",
        "- $\\mathcal{T}$ treino (0.8 * $\\mathcal{D}$)\n",
        "- $\\mathcal{V}$ validação, ou _holdout validation_ (0.2 * $\\mathcal{T}$)\n",
        "- $\\mathcal{T'}$ teste (0.2 * $\\mathcal{D}$)\n",
        "\n",
        "$\\mathcal{V}$ pode ser usado para avaliar vários modelos candidatos. O fluxo então é\n",
        "\n",
        "1. $M_1, M_2, M_3 \\rightarrow \\mathcal{T}$\n",
        "2. $M_1, M_2, M_3 \\rightarrow \\mathcal{V} \\Rightarrow M^{*}$ \n",
        "3. $M^{*} \\rightarrow \\mathcal{T'}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2wQgpxUf7Vb",
        "colab_type": "text"
      },
      "source": [
        "### Cross-Validation\n",
        "\n",
        "Dentre as infinitas formas de definir: modelo complexo $\\to$ ruido capturado  $\\to$ overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj6gDVsZf7gk",
        "colab_type": "text"
      },
      "source": [
        "## Cost vs Loss vs Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_-GbTpG02PR",
        "colab_type": "text"
      },
      "source": [
        "+ **Loss function** is usually a function **defined on a data point**, prediction and label, and measures the penalty. Ex: square loss @ linear regression.\n",
        "\n",
        "+ **Cost function** is usually more general. It might be a **sum of loss functions over your training** set plus some model complexity penalty (regularization). Ex: mse @ linear regression\n",
        "\n",
        "+ **Objective function** is the most general term for **any function that you optimize during training**. Ex: mle @ linear regression \n",
        "\n",
        "[ref](https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deoTThFSf790",
        "colab_type": "text"
      },
      "source": [
        "## Distance Metrics\n",
        "\n",
        "![](https://raw.githubusercontent.com/thiagodsd/thiagodsd.github.io/master/img/index.png)\n",
        "\n",
        "+ Chebyshev\n",
        "+ [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)\n",
        "+ [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448WxDuwSFwm",
        "colab_type": "text"
      },
      "source": [
        "## Performance Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7n2LWutXOVl",
        "colab_type": "text"
      },
      "source": [
        "Identificando, modelo $h$ tal que $h(x) = \\hat{y}$ e a predicao para $y$ dado $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MxSOZISVSlD",
        "colab_type": "text"
      },
      "source": [
        "### Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2KDdBbqXIDM",
        "colab_type": "text"
      },
      "source": [
        "* MAE, mean absolute error $$\\frac{1}{N}\\sum_i^N |y_i - h(x)|$$\n",
        "\n",
        "* MSE, mean squared error $$\\frac{1}{N}\\sum_i^N (y_i - h(x))^2$$\n",
        "\n",
        "* RMSE, root mean squared error $$\\sqrt{\\frac{1}{N}\\sum_i^N (y_i - h(x))^2}=\\sqrt{\\text{MSE}}$$\n",
        "\n",
        "* $R^2$, coefficient of determination $$1 - \\frac{\\sum (y_i - h(x))^2}{\\sum(y_i  - \\bar{y})^2}$$\n",
        "\n",
        "* $\\bar{R}^2$, adjusted coefficient of determination, para $k$ features e $n$ linhas $$1 - (1-R^2)\\frac{n-1}{n-(k+1)}$$\n",
        "\n",
        "- - -\n",
        "\n",
        "Como sempre -- devido $\\ell_1$ vs $\\ell_2$ -- MAE melhor que RMSE caso o conjunto tenha muitos outliers. Por outro lado MAE envolve calculo de modulo, que geralmente e caro.\n",
        "\n",
        "Particularidade da RMSE: ela e sensivel a variancia da distribuicao de magnitude dos erros -- o que e diferente de ser sensivel a variancia dos erros. Portanto RMSE **nao mede apenas o erro**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvBOwX8EWo6A",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50BH8hzdWqQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}